{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55416ea",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import wordcloud\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import gensim.models.ldamodel\n",
    "import pyLDAvis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import time\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from gensim.model import CoherenceModel\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# modelling imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress DeprecationWarning from Pillow\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332e8e1",
   "metadata": {},
   "source": [
    "# Read data\n",
    "This class is use to: \n",
    "\n",
    "    -read the csv file on the given data and the filipino stopwords\n",
    "    -pre-clean the data by dropping unnecesary columns and null values.\n",
    "    -split the filipino text and english text\n",
    "    \n",
    "    The parameters are:\n",
    "        -filename (string format)\n",
    "        -stopwords (string format) (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93e751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class base:\n",
    "    def __init__(self, df: str, added_stopwords:str = None):\n",
    "        self.stop_words_filename = added_stopwords\n",
    "        self.df_filename = df\n",
    "        self.df = None\n",
    "        self.stopwords_df = None\n",
    "        self.stopwords = []\n",
    "        \n",
    "        # read file\n",
    "        self.reading()\n",
    "        \n",
    "    def reading(self):\n",
    "        # read stopwords csv\n",
    "        if stopwords is not None:\n",
    "            self.stopwords_df = pd.read_csv(self.stop_words_filename)\n",
    "            self.stopwords = [word for word in self.stopwords_df['.stopwords']]\n",
    "        else:\n",
    "            self.stopwords_df = None\n",
    "        # read csv file\n",
    "        self.df = pd.read_csv(self.df_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ce117",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess(base):\n",
    "    def __init__(self, df, added_stopwords, sent_column: list, comment_column: str, dups: int = 0, drop : list = None):\n",
    "        self.columns_to_drop = drop\n",
    "        self.sents = sent_column\n",
    "        self.column = comment_column\n",
    "        self.dups = dups\n",
    "        super().__init__(df, added_stopwords)\n",
    "        self.stop_words = stopwords.words('english')       # words to stop\n",
    "        self.stopwords.extend(self.stop_words)             # combine all the stopwords\n",
    "        self.tokenizer = RegexpTokenizer(r\"\\w+|[^\\w\\s]+\")  # tokenizer\n",
    "        self.substituted_text = []                         # empty list of to be substituted text\n",
    "        self.initial_tokens = []                           # empty list of initial tokens\n",
    "        self.stopped_tokens = []                           # empty list of tokens without the stopwords\n",
    "        self.stopped_text = []                             # empty list of tokens in string format\n",
    "        self.sentiment_dict = {                            # switch name into integers\n",
    "            'nagative': 0,\n",
    "            'positive': 2,\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positve': 2,\n",
    "            'neitral': 1,\n",
    "            'posiive': 2,\n",
    "            'Negative': 0,\n",
    "            'Neutral': 1,\n",
    "            'Positive': 2,\n",
    "            0.0: 0,\n",
    "            1.0: 1,\n",
    "            2.0: 2,\n",
    "        }\n",
    "        \n",
    "        # Map the categories to numbers\n",
    "        for sentiments_column in self.sents:\n",
    "            self.df[sentiments_column] = self.df[sentiments_column].map(self.sentiment_dict)\n",
    "        \n",
    "        # drop unnecesary columns\n",
    "        if self.columns_to_drop != None:\n",
    "            self.df.drop(self.columns_to_drop, axis = 1, inplace = True) \n",
    "        \n",
    "        # preprocess\n",
    "        self.df.dropna(subset=[self.column], inplace=True)\n",
    "        print(\"shape of dataframe: \", self.df.shape)\n",
    "        self.preprocessing_steps()\n",
    "        \n",
    "        # append to dataframe\n",
    "        self.df[\"features\"] = self.stopped_tokens\n",
    "        self.df[\"features_string_format\"] = self.stopped_text\n",
    "        \n",
    "        # drop null values in the comment\n",
    "        self.df.dropna(subset=[self.column], inplace=True)\n",
    "        print(\"shape of dataframe: \", self.df.shape)\n",
    "        \n",
    "        # drop duplicated values\n",
    "        if self.dups == 1:\n",
    "            print('drop')\n",
    "            self.df = self.df.drop_duplicates(subset = \"features_string_format\")\n",
    "        \n",
    "        # drop rows that have no values\n",
    "        self.df.drop(self.df[self.df['features_string_format'] == ''].index, inplace=True)\n",
    "\n",
    "        \n",
    "    def preprocessing_steps(self):\n",
    "        # substitute the unnecesary text to \"\"\n",
    "        self.substituted_text = [self.substitute(text).lower() for text in self.df[self.column]]\n",
    "        \n",
    "        # tokenize the text\n",
    "        self.initial_tokens = [self.tokenizer.tokenize(text) for text in self.substituted_text]\n",
    "        \n",
    "        # remove stopwords\n",
    "        self.stopped_tokens = [[token for token in token_list if len(token) > 2 and token not in self.stopwords] for token_list in self.initial_tokens]\n",
    "        \n",
    "        # make them in string format\n",
    "        self.stopped_text = [' '.join(tokens) for tokens in self.stopped_tokens]\n",
    "        \n",
    "        \n",
    "    def substitute(self, text):\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        text = re.sub(r'\\S*@\\S*', '', text)\n",
    "        text = re.sub(r'\\b(?:http\\S+|@\\S+)\\b', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\?\\!\\.\\,]', '', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\?\\!\\.\\,]', '', text)\n",
    "        text = re.sub(r'\\b\\w*haha\\w*\\b', 'hahahaha', text)\n",
    "        text = re.sub(r'bomata', 'bumata', text)\n",
    "        text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "        text = re.sub(r'bbo', 'bobo', text)\n",
    "        text = re.sub(r'tnga', 'tanga', text)\n",
    "        text = re.sub(r'pnget', 'panget', text)\n",
    "        text = re.sub(r'thanks', 'thank', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cea4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = preprocess(df='balanced_data_context6k.csv',added_stopwords='filipino_stopwords.csv', \n",
    "                                       sent_column=['context'], \n",
    "                                       comment_column='Comment', dups=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfecd3",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This class is use to: \n",
    "\n",
    "    -explore the data\n",
    "    -find insights within the data \n",
    "    -outputs the visualization that would be useful for preprocessing and presentation purposes.\n",
    "\n",
    "    The following are the outputs:\n",
    "        -descriptions (i.e. categories, number of values, mean)\n",
    "        -wordcloud\n",
    "        -sentiment plots\n",
    "        -bar plots\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eda:\n",
    "    def __init__(self, df, text_column_name: str, sent_column_name: str):\n",
    "        self.text_column_name = text_column_name\n",
    "        self.sent_column_name = sent_column_name\n",
    "        self.df = df\n",
    "        self.wordcloud = WordCloud(width=1200, height=800, max_words=500, background_color=\"white\", scale=2)\n",
    "        self.all_bigrams = []\n",
    "        self.all_text = ' '.join(self.df[self.text_column_name])\n",
    "        self.background_color = '#adadadff'\n",
    "        self.text_color = '#000000'\n",
    "        self.bar_color = '#121166'\n",
    "\n",
    "    def descriptions(self):\n",
    "        print('shape:\\n', self.df.shape, '\\n')\n",
    "        print('sentiment count:\\n', self.df[self.sent_column_name].value_counts(), '\\n')\n",
    "        print('columns:', list(self.df.columns.values), '\\n')\n",
    "\n",
    "    def word_number(self):\n",
    "        word_counts = self.df[self.text_column_name].str.split().apply(len)\n",
    "        bins = np.arange(1, max(word_counts) + 2)  # Start from 1 and end at max + 1\n",
    "        plt.hist(word_counts, bins=bins)\n",
    "        plt.xlabel('Number of Words')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Number of Words in Comments')\n",
    "\n",
    "        # Customize x-axis tick positions and labels\n",
    "        tick_positions = np.arange(0, 30, 5)  # Start from 1, increment by 5, end at 30\n",
    "        tick_labels = [str(pos) for pos in tick_positions]\n",
    "        plt.xticks(tick_positions, tick_labels)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def top_words(self, top: int, sentiment: str, filename: str):\n",
    "        # Tokenize the text\n",
    "        tokens = self.all_text.split()\n",
    "\n",
    "        # Count the frequency of each word\n",
    "        word_counts = pd.Series(tokens).value_counts()\n",
    "\n",
    "        # Select the top 10 words\n",
    "        top_10_words = word_counts.head(top)\n",
    "\n",
    "        # Set the color specifications\n",
    "        background_color = self.background_color\n",
    "        text_color = self.text_color\n",
    "        bar_color = self.bar_color\n",
    "\n",
    "        # Create the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(11, 7), facecolor=background_color)\n",
    "        ax.barh(top_10_words.index, top_10_words.values, color=bar_color)\n",
    "\n",
    "        # Set the background and text colors\n",
    "        fig.set_facecolor(background_color)\n",
    "        ax.set_facecolor(background_color)\n",
    "        ax.xaxis.label.set_color(text_color)\n",
    "        ax.yaxis.label.set_color(text_color)\n",
    "        ax.tick_params(axis='x', colors=text_color)\n",
    "        ax.tick_params(axis='y', colors=text_color)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Frequency', color=text_color)\n",
    "        plt.ylabel('Words', color=text_color)\n",
    "        plt.title('Top {} {} Words'.format(top, sentiment), color=text_color)\n",
    "\n",
    "        # Show the chart\n",
    "        plt.savefig('top_{}words_{}.png'.format(top, filename))\n",
    "        plt.show()\n",
    "        \n",
    "    def bigrams(self, top: int, sentiment: str, filename: str):\n",
    "        # Iterate over each text and compute bigrams\n",
    "        for text in self.df[self.text_column_name]:\n",
    "            # Tokenize the text into individual words\n",
    "            tokens = text.split()\n",
    "\n",
    "            # Generate bigrams\n",
    "            bigram_tokens = list(combinations(tokens, 2))\n",
    "\n",
    "            # Add the bigrams to the list\n",
    "            self.all_bigrams.extend(bigram_tokens)\n",
    "\n",
    "        # Count the frequency of each bigram\n",
    "        bigram_counts = Counter(self.all_bigrams)\n",
    "\n",
    "        # Select the top 10 bigrams\n",
    "        top_10_bigrams = bigram_counts.most_common(top)\n",
    "\n",
    "        # Extract the bigram phrases and their frequencies\n",
    "        bigram_phrases, bigram_frequencies = zip(*top_10_bigrams)\n",
    "\n",
    "        # Convert the tuples to lists\n",
    "        bigram_phrases = [', '.join(phrase) for phrase in bigram_phrases]\n",
    "        bigram_frequencies = list(bigram_frequencies)\n",
    "\n",
    "        # Set the color specifications\n",
    "        background_color = self.background_color\n",
    "        text_color = self.text_color\n",
    "        bar_color = self.bar_color\n",
    "\n",
    "        # Create the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(10, 8), facecolor=background_color)\n",
    "        ax.barh(bigram_phrases, bigram_frequencies, color=bar_color)\n",
    "\n",
    "        # Set the background and text colors\n",
    "        fig.set_facecolor(background_color)\n",
    "        ax.set_facecolor(background_color)\n",
    "        ax.xaxis.label.set_color(text_color)\n",
    "        ax.yaxis.label.set_color(text_color)\n",
    "        ax.tick_params(axis='x', colors=text_color)\n",
    "        ax.tick_params(axis='y', colors=text_color)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Frequency', color=text_color)\n",
    "        plt.ylabel('Bigrams', color=text_color)\n",
    "        plt.title('Top {} {} Bigrams'.format(top, sentiment), color=text_color)\n",
    "\n",
    "        # Show the chart\n",
    "        plt.savefig('top_{}bigrams_{}.png'.format(top, filename))\n",
    "        plt.show()\n",
    "        \n",
    "    def english_tagalog(self):\n",
    "        # Count the occurrences of each language\n",
    "        language_counts = df['language'].value_counts()\n",
    "        \n",
    "        # Plotting the bar graph\n",
    "        plt.bar(language_counts.index, language_counts.values)\n",
    "\n",
    "        # Adding labels and title\n",
    "        plt.xlabel('Language')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Language Distribution')\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    def generate_wordcloud(self, filename: str, category: str = None, value: int = None):\n",
    "        if category is not None:\n",
    "            if value is None:\n",
    "                print(\"Input category value.\")\n",
    "                return\n",
    "            joined_comments = ''\n",
    "            for index, row in self.df.iterrows():\n",
    "                if row[category] == value:\n",
    "                    joined_comments += ' ' + str(row[self.text_column_name])\n",
    "            self.wordcloud.generate(joined_comments)\n",
    "        else:\n",
    "            joined_comments = ' '.join(self.df[self.text_column_name])\n",
    "            self.wordcloud.generate(joined_comments)\n",
    "\n",
    "        # Set the background color\n",
    "        background_color = self.background_color\n",
    "        self.wordcloud.background_color = background_color\n",
    "\n",
    "        # Set the colors of the words\n",
    "        word_colors = ['#b31212ff', '#121cb3ff', '#a51064ff', '#14630dff']\n",
    "        self.wordcloud.recolor(color_func=lambda *args, **kwargs: random.choice(word_colors))\n",
    "\n",
    "        self.wordcloud.to_file('{}_wordcloud.png'.format(filename))\n",
    "        plt.imshow(self.wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    def pie_sentiment(self, filename, column_name):\n",
    "        value_counts = self.df[column_name].value_counts()\n",
    "\n",
    "        labels = value_counts.index\n",
    "        counts = value_counts.values\n",
    "\n",
    "        colors = ['#001C30', '#176B87', '#64CCC5']\n",
    "        background_color = self.background_color\n",
    "\n",
    "        # Map the index values to the desired labels\n",
    "        sentiment_labels = labels.map({0.0: 'negative', 1.0: 'neutral', 2.0: 'positive'})\n",
    "\n",
    "        plt.figure(facecolor=background_color)\n",
    "        plt.style.use('default')\n",
    "        plt.pie(counts, labels=sentiment_labels, autopct='%1.1f%%', colors=colors)\n",
    "        plt.axis('equal')\n",
    "        plt.gca().set_facecolor(background_color)\n",
    "        plt.savefig('{}_pie_plot_sentiment.png'.format(filename))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def bar_sentiment(self, filename: str, column_name: str):\n",
    "        # Set the color specifications\n",
    "        background_color = self.background_color\n",
    "        text_color = self.text_color\n",
    "        bar_color = self.bar_color\n",
    "\n",
    "        # Set the background color\n",
    "        plt.figure(facecolor=background_color)\n",
    "\n",
    "        # Count the occurrences of each sentiment\n",
    "        sentiment_counts = self.df[column_name].value_counts()\n",
    "\n",
    "        # Define the color specifications\n",
    "        color_mapping = {'negative': '#D21312', 'positive': '#03C988', 'neutral': '#576CBC'}\n",
    "\n",
    "        # Map the index values to the desired labels\n",
    "        sentiment_labels = sentiment_counts.index.map({0.0: 'negative', 1.0: 'neutral', 2.0: 'positive'})\n",
    "\n",
    "        # Create a bar plot with custom colors and updated labels\n",
    "        plt.bar(sentiment_labels, sentiment_counts.values, color=[color_mapping.get(sentiment, bar_color) for sentiment in sentiment_labels])\n",
    "\n",
    "        # Set the text color\n",
    "        plt.xlabel('Sentiments', color=text_color)\n",
    "        plt.ylabel('Count', color=text_color)\n",
    "\n",
    "        # Set the title and text colors based on column_name\n",
    "        if column_name == 'Comment type':\n",
    "            plt.title('Given Sentiments', color=text_color)\n",
    "        elif column_name == 'sentiments':\n",
    "            plt.title('Tagalog & English Sentiments', color=text_color)\n",
    "        elif column_name == 'english_sents':\n",
    "            plt.title('English Sentiments', color=text_color)\n",
    "\n",
    "        # Set the tick colors\n",
    "        plt.tick_params(colors=text_color)\n",
    "\n",
    "        # Set the plot area color to the same as the background color\n",
    "        plt.gca().set_facecolor(background_color)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.savefig('{}_sentiment_plot.png'.format(filename))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb02229",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "    this class is designed to use a pre-trained model to determine wether a text is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3af90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.fasttext_model = fasttext.load_model('C:\\\\project\\\\downloads\\\\lid.176.ftz')\n",
    "        \n",
    "        self.do_sentiments()\n",
    "        \n",
    "    def do_sentiments(self):\n",
    "        # declare a list of sentiments\n",
    "        english_sentiments = []\n",
    "        \n",
    "        for index, row in self.df.iterrows():\n",
    "            if row['language'] == \"English\":\n",
    "                # Perform sentiment analysis with VADER\n",
    "                vader_sentiment = self.vader_analyzer.polarity_scores(row['features_string_format'])\n",
    "                # Get the most probable sentiment label\n",
    "                sentiment_label = max(vader_sentiment, key=vader_sentiment.get)\n",
    "                # Map the sentiment labels to custom labels\n",
    "                if sentiment_label == \"neu\":\n",
    "                    sentiment_label = \"neutral\"\n",
    "                elif sentiment_label == \"pos\" or sentiment_label == \"compound\":\n",
    "                    sentiment_label = \"positive\"\n",
    "                else:\n",
    "                    sentiment_label = \"negative\"\n",
    "                # Update the dataframe with sentiment label\n",
    "                english_sentiments.append(sentiment_label)\n",
    "            else:\n",
    "                english_sentiments.append(None)\n",
    "        \n",
    "        # append to dataframe\n",
    "        self.df['english_sents'] = english_sentiments\n",
    "        \n",
    "    def combine_sentiments(self):\n",
    "        english_tagalog_sents = []\n",
    "        for index, row in self.df.iterrows():\n",
    "            if row['language'] == 'English':\n",
    "                english_tagalog_sents.append(row['english_sents'])\n",
    "            else:\n",
    "                english_tagalog_sents.append(row['Comment type'])\n",
    "        self.df['sentiments'] = english_tagalog_sents\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f18d86",
   "metadata": {},
   "source": [
    "# Unsupervised using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0f7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "class LDA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.lowered_token_list = []\n",
    "        self.corpus = []\n",
    "        self.model = None\n",
    "        self.modelling()\n",
    "        \n",
    "    def preparing(self):\n",
    "        # lowercase the tokens\n",
    "        self.lowered_token_list = []\n",
    "        \n",
    "        # put the values into the empty list\n",
    "        for index, row in self.df.iterrows():\n",
    "            lowercase_tokens = []\n",
    "            for token in row['features']:\n",
    "                lowercase_tokens.append(token.lower())\n",
    "            self.lowered_token_list.append(lowercase_tokens)\n",
    "        \n",
    "        # initialize id2word here\n",
    "        self.id2word = corpora.Dictionary(self.lowered_token_list)\n",
    "        \n",
    "        # put the tokens into the corpus\n",
    "        for list_of_tokens in self.lowered_token_list:\n",
    "            new = self.id2word.doc2bow(list_of_tokens)\n",
    "            self.corpus.append(new)\n",
    "    \n",
    "    def modelling(self):\n",
    "        self.preparing()\n",
    "        self.model = models.LdaModel(\n",
    "            corpus=self.corpus,\n",
    "            id2word=self.id2word,\n",
    "            num_topics=3,\n",
    "            random_state=42,\n",
    "            update_every=1,\n",
    "            passes=10,\n",
    "            chunksize=180,\n",
    "            alpha='auto'\n",
    "        )\n",
    "    \n",
    "    def output(self):\n",
    "        pyLDAvis.enable_notebook()\n",
    "        vis = pyLDAvis.gensim_models.prepare(self.model, self.corpus, self.id2word, mds='mmds', R=30)\n",
    "        pyLDAvis.save_html(vis, 'visualization.html')  # Save the visualization to an HTML file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a082db",
   "metadata": {},
   "source": [
    "# Unsupervised using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(df['features_string_format'])\n",
    "        \n",
    "        self.min_clusters = 1\n",
    "        self.max_clusters = 6\n",
    "        self.cluster_range = range(self.min_clusters, self.max_clusters)\n",
    "        self.inertia_values = []\n",
    "        self.silhouette_scores = []\n",
    "        \n",
    "        self.find_best_cluster()\n",
    "        self.plot()\n",
    "        \n",
    "    def find_best_cluster(self):\n",
    "        for num_clusters in self.cluster_range:\n",
    "            kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "            kmeans.fit(self.tfidf_matrix)\n",
    "            self.inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "            labels = kmeans.labels_\n",
    "            num_unique_labels = len(np.unique(labels))\n",
    "\n",
    "            if num_unique_labels > 1:\n",
    "                score = silhouette_score(self.tfidf_matrix, labels)\n",
    "                self.silhouette_scores.append(score)\n",
    "            else:\n",
    "                self.silhouette_scores.append(0.0)\n",
    "        \n",
    "    def plot(self):\n",
    "        if not self.cluster_range or not self.inertia_values or not self.silhouette_scores:\n",
    "            print(\"Insufficient data to plot.\")\n",
    "            return\n",
    "    \n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.plot(self.cluster_range, self.inertia_values, marker='o', color='green')\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Inertia')\n",
    "        ax1.set_title('Elbow Curve')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(self.cluster_range, self.silhouette_scores, marker='o', color='red')\n",
    "        ax2.set_ylabel('Silhouette Score')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def update_database(self, best_cluster):\n",
    "        # cluster\n",
    "        kmeans = KMeans(n_clusters=best_cluster, init='k-means++', random_state=42)\n",
    "        kmeans.fit(self.tfidf_matrix)\n",
    "        \n",
    "        # update dataframe\n",
    "        self.df['cluster_label'] = kmeans.labels_.astype(str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class models:\n",
    "    def __init__(self, df, x: str, y: str, filename: str):\n",
    "        self.filename = filename\n",
    "        self.df = df\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.xtrain, self.xtest, self.ytrain, self.ytest = train_test_split(self.df[self.x], self.df[self.y], \n",
    "                                                                            test_size=.20, stratify=df[self.y])\n",
    "        self.xtrain_vectors = self.vectorizer.fit_transform(self.xtrain)\n",
    "        self.xtest_vectors = self.vectorizer.transform(self.xtest)\n",
    "        self.ytest = self.ytest.astype(int)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.ytrain)\n",
    "        self.model_name = None\n",
    "        self.y_predicted = None\n",
    "        \n",
    "    def bert_classification(self, epochs: int):\n",
    "        self.model_name = 'bert'\n",
    "        # Model framework\n",
    "        e_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "        p_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "        \n",
    "        bert_preprocess = hub.KerasLayer(p_url)\n",
    "        bert_encoder = hub.KerasLayer(e_url)\n",
    "        \n",
    "        article = tf.keras.layers.Input(shape=(), dtype=tf.string, name='article')\n",
    "        p_text = bert_preprocess(article)\n",
    "        output = bert_encoder(p_text)\n",
    "        l = tf.keras.layers.BatchNormalization()(output['pooled_output'])\n",
    "        l = tf.keras.layers.Dropout(0.1, name='dropout_1')(l)\n",
    "        l = tf.keras.layers.Dense(128, activation='relu')(l)\n",
    "        l = tf.keras.layers.Dropout(0.1, name='dropout')(l)\n",
    "        l = tf.keras.layers.Dense(3, activation='softmax', name='output')(l)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[article], outputs=[l])\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                           loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Start the time checker of the program\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(self.xtrain, self.ytrain, epochs=epochs)\n",
    "        \n",
    "        # End time\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Print the elapsed time\n",
    "        print('Elapsed time:', elapsed_time / 60)\n",
    "        \n",
    "        self.model.save('bert_model_{}.h5'.format(self.filename))\n",
    "        \n",
    "    def svm(self):\n",
    "        self.model_name = 'svm'\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],  # Penalty parameter C\n",
    "            'kernel': ['linear', 'rbf', 'poly'],  # Kernel type\n",
    "            'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf' and 'poly' kernels\n",
    "        }\n",
    "        clf_svm = svm.SVC(kernel='linear')\n",
    "        \n",
    "        # Instantiate GridSearchCV\n",
    "        grid_search = GridSearchCV(clf_svm, param_grid, cv=10)  # 5-fold cross-validation\n",
    "        \n",
    "        # Fit the gridsearch to the data\n",
    "        grid_search.fit(self.xtrain_vectors, self.ytrain)\n",
    "        \n",
    "        # Access the best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        # Create a new SVM model with the best parameters\n",
    "        best_svm_model = svm.SVC(**best_params)\n",
    "    \n",
    "        # Train the best SVM model on the entire training data\n",
    "        best_svm_model.fit(self.xtrain_vectors, self.ytrain)\n",
    "\n",
    "        # save model to class\n",
    "        self.model = best_svm_model\n",
    "        \n",
    "        \n",
    "        # Save the model to a file\n",
    "        joblib.dump(self.model, 'svm_model_{}.pkl'.format(self.filename))\n",
    "        \n",
    "        # Save the vectors\n",
    "        self.xtrain.to_csv('{}_{}.csv'.format(self.model_name, self.filename))\n",
    "        \n",
    "        # Save the label encoder\n",
    "        joblib.dump(self.label_encoder, 'svm_label_encoder_{}.pkl'.format(self.filename))\n",
    "        \n",
    "    def decision_tree(self):\n",
    "        self.model_name = 'dectree'\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {\n",
    "            'criterion': ['gini', 'entropy'],         # Split criterion\n",
    "            'splitter': ['best', 'random'],          # Strategy for choosing split\n",
    "            'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
    "            'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "            'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "            'max_features': ['sqrt', 'log2'] # Number of features to consider for the best split\n",
    "        }\n",
    "\n",
    "        # Create the Decision Tree model\n",
    "        dt_model = DecisionTreeClassifier()\n",
    "\n",
    "        # Instantiate GridSearchCV\n",
    "        grid_search = GridSearchCV(dt_model, param_grid, cv=10)  # 5-fold cross-validation\n",
    "\n",
    "        # Fit GridSearchCV to the data\n",
    "        grid_search.fit(self.xtrain_vectors, self.ytrain)\n",
    "\n",
    "        # Access the best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Create a new Decision Tree model with the best parameters\n",
    "        best_dt_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "        # Train the best Decision Tree model on the entire training data\n",
    "        best_dt_model.fit(self.xtrain_vectors, self.ytrain)\n",
    "        \n",
    "        self.model = best_dt_model\n",
    "        \n",
    "        # Save the model to a file\n",
    "        joblib.dump(self.model, 'dectree_model_{}.pkl'.format(self.filename))\n",
    "        \n",
    "        # Save the vectors\n",
    "        self.xtrain.to_csv('{}_{}.csv'.format(self.model_name, self.filename))\n",
    "        \n",
    "        # Save the label encoder\n",
    "        joblib.dump(self.label_encoder, 'dectree_label_encoder_{}.pkl'.format(self.filename))\n",
    "        \n",
    "    def random_forest(self):\n",
    "        self.model_name = 'rf'\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],          # Number of trees in the forest\n",
    "            'criterion': ['gini', 'entropy'],         # Split criterion for individual trees\n",
    "            'max_depth': [None, 10, 20, 30],         # Maximum depth of the individual trees\n",
    "            'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "            'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "            'max_features': ['sqrt', 'log2'] # Number of features to consider for the best split\n",
    "        }\n",
    "\n",
    "        # Create the Random Forest model\n",
    "        rf_model = RandomForestClassifier()\n",
    "\n",
    "        # Instantiate GridSearchCV\n",
    "        grid_search = GridSearchCV(rf_model, param_grid, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "        # Fit GridSearchCV to the data\n",
    "        grid_search.fit(self.xtrain_vectors, self.ytrain)\n",
    "\n",
    "        # Access the best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Create a new Random Forest model with the best parameters\n",
    "        best_rf_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "        # Train the best Random Forest model on the entire training data\n",
    "        best_rf_model.fit(self.xtrain_vectors, self.ytrain)\n",
    "        \n",
    "        self.model = best_rf_model\n",
    "        \n",
    "        # Save the model to a file\n",
    "        joblib.dump(self.model, 'random_forest_model_{}.pkl'.format(self.filename))\n",
    "        \n",
    "        # Save the vectors\n",
    "        self.xtrain.to_csv('{}_{}.csv'.format(self.model_name, self.filename))\n",
    "        \n",
    "        # Save the label encoder\n",
    "        joblib.dump(self.label_encoder, 'random_forest_label_encoder_{}.pkl'.format(self.filename))\n",
    "\n",
    "\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        # if there are no models\n",
    "        if self.model_name is None:\n",
    "            print('You need to train a model first')\n",
    "            return\n",
    "        \n",
    "        # if a bert model was used\n",
    "        if self.model_name == 'bert':\n",
    "            print('bert')\n",
    "            prediction = self.model.predict(self.xtest)\n",
    "            target_names = ['1.0', '2.0', '0']\n",
    "            y_preds = np.argmax(prediction, axis=1)\n",
    "            print(\"Classification Report: \\n\", classification_report(self.ytest, y_preds, target_names=target_names))\n",
    "        # else if a svm or decision tree model was used\n",
    "        elif self.model_name in [\"svm\", \"dectree\", \"rf\"]:\n",
    "            print('svm or dectree')\n",
    "            predictions = self.model.predict(self.xtest_vectors)\n",
    "            accuracy = accuracy_score(self.ytest, predictions)\n",
    "            f1_micro = f1_score(self.ytest, predictions, average='micro')\n",
    "            f1_macro = f1_score(self.ytest, predictions, average='macro')\n",
    "            classification_rep = classification_report(self.ytest, predictions)\n",
    "            \n",
    "            # Print scores and classification report\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"F1 score (micro):\", f1_micro)\n",
    "            print(\"F1 score (macro):\", f1_macro)\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_rep)\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            cm = confusion_matrix(self.ytest, predictions)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.xlabel('Predicted Labels')\n",
    "            plt.ylabel('Actual Labels')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('The models that can be used are \"bert\", \"svm\", \"dectree\", \"rf\". Choose one.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa32f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
