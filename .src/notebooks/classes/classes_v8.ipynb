{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55416ea",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8911d3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mdfl0\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mdfl0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import wordcloud\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import gensim.models.ldamodel\n",
    "import pyLDAvis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import time\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from gensim.model import CoherenceModel\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# modelling imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cac80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress DeprecationWarning from Pillow\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332e8e1",
   "metadata": {},
   "source": [
    "# Read data\n",
    "This class is use to: \n",
    "\n",
    "    -read the csv file on the given data and the filipino stopwords\n",
    "    -pre-clean the data by dropping unnecesary columns and null values.\n",
    "    -split the filipino text and english text\n",
    "    \n",
    "    The parameters are:\n",
    "        -filename (string format)\n",
    "        -stopwords (string format) (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ce117",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9778e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self, df, comment_column: str, dups: int = 0, use_for: int = 1, dup_subset: list = None, added_stopwords = None, drop : list = None, sent_column: list = None):\n",
    "        self.use_for = use_for\n",
    "        self.columns_to_drop = drop\n",
    "        self.sents = sent_column\n",
    "        self.column = comment_column\n",
    "        self.dups = dups\n",
    "        self.dup_subset = dup_subset\n",
    "        self.filipino_stopwords = []\n",
    "        # read data\n",
    "        if type(df) == str:\n",
    "            self.df = pd.read_csv(df)\n",
    "        elif type(df) == pd.core.frame.DataFrame:\n",
    "            self.df = df\n",
    "        else:\n",
    "            print('Input a filename or the dataframe.')\n",
    "            return\n",
    "        \n",
    "        # read added stopwords\n",
    "        if added_stopwords != None:\n",
    "            stopwords_df = pd.read_csv(added_stopwords)\n",
    "            self.filipino_stopwords = [word for word in stopwords_df['.stopwords']]\n",
    "        \n",
    "        # print columns\n",
    "        column_headers = list(self.df.columns.values)\n",
    "        print(\"Column Headers: \", column_headers)\n",
    "        self.stop_words = self.filipino_stopwords          # words to stop  \n",
    "        self.tokenizer = RegexpTokenizer(r\"\\w+|[^\\w\\s]+\")  # tokenizer\n",
    "        self.substituted_text = []                         # empty list of to be substituted text\n",
    "        self.initial_tokens = []                           # empty list of initial tokens\n",
    "        self.stopped_tokens = []                           # empty list of tokens without the stopwords\n",
    "        self.stopped_text = []                             # empty list of tokens in string format\n",
    "        \n",
    "        if self.sents is not None:\n",
    "            self.sentiment_dict = {                            # switch name into integers\n",
    "                'nagative': 0,\n",
    "                'positive': 2,\n",
    "                'negative': 0,\n",
    "                'neutral': 1,\n",
    "                'neitral': 1,\n",
    "                'positve': 2,\n",
    "                'posiive': 2,\n",
    "                'neitral': 1,\n",
    "                'posiive': 2,\n",
    "                'Negative': 0,\n",
    "                'Neutral': 1,\n",
    "                'Positive': 2,\n",
    "                'Postive': 2,\n",
    "                'positve': 2,\n",
    "                'neiutral': 1,\n",
    "                'nuetral': 1,\n",
    "                'neutrl': 1,\n",
    "                'neutrsal': 1,\n",
    "                '1':1,\n",
    "                '2':2,\n",
    "                '0':0,\n",
    "                0.0: 0,\n",
    "                1.0: 1,\n",
    "                2.0: 2\n",
    "            }\n",
    "        \n",
    "        # show raw dataframe\n",
    "        print(\"shape of raw dataframe: \", self.df.shape)\n",
    "        \n",
    "        # if there are no sentiment columns (yet)\n",
    "        if self.sents is not None:\n",
    "            # Drop Null Values\n",
    "            for sent_column in self.sents:\n",
    "                self.df.dropna(subset=[sent_column], inplace=True)\n",
    "                \n",
    "            # Map the categories to numbers\n",
    "            print(\"Sentiment null count: \", self.df[self.sents].isnull().sum())\n",
    "            for sentiments_column in self.sents:\n",
    "                self.df[sentiments_column] = self.df[sentiments_column].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "                print(self.df[self.sents].value_counts())\n",
    "                print(\"shape of mapped dataframe: \", self.df.shape)\n",
    "\n",
    "            for sentiments_column in self.sents:\n",
    "                self.df[sentiments_column] = self.df[sentiments_column].apply(lambda x: x.lower() if isinstance(x, str) else x).map(self.sentiment_dict)\n",
    "                print(\"Sentiment null count: \", self.df[self.sents].isnull().sum())\n",
    "                \n",
    "            if sentiments_column != 'sentiments':\n",
    "                self.df['sentiments'] = self.df[sentiments_column]\n",
    "                self.df.drop([sentiments_column], axis = 1, inplace = True) \n",
    "        \n",
    "        # preprocess\n",
    "        self.df.dropna(subset=[self.column], inplace=True)\n",
    "        self.preprocessing_steps()\n",
    "        \n",
    "        # append to dataframe\n",
    "        self.df[\"features\"] = self.stopped_tokens\n",
    "        self.df[\"features_string_format\"] = self.stopped_text\n",
    "        \n",
    "        # drop null values in the comment\n",
    "        self.df.dropna(subset=[self.column], inplace=True)\n",
    "        print(\"shape of dataframe when null comments were dropped: \", self.df.shape)\n",
    "        \n",
    "        # drop duplicated values\n",
    "        if self.dups == 1:\n",
    "            if self.dup_subset == None:\n",
    "                print(\"You need to input the subsets of the duplicate values.\")\n",
    "                return\n",
    "            if not self.dup_subset:\n",
    "                print(\"List is empty. Input columns in the 'dup_subset' variable.\")\n",
    "                return\n",
    "            self.df.dropna(subset=self.dup_subset, inplace=True)\n",
    "            self.df = self.df.drop_duplicates(subset=self.dup_subset)\n",
    "            print(\"shape of dataframe when preprocessed and duplicated values where dropped: \", self.df.shape)\n",
    "            \n",
    "        # drop unnecesary columns\n",
    "        if self.columns_to_drop != None:\n",
    "            self.df.drop(self.columns_to_drop, axis = 1, inplace = True) \n",
    "        \n",
    "        # drop rows that have no values (if for preprocessing data (1), else for inputting neutral sentiments (0))\n",
    "        if self.use_for == 1:\n",
    "            self.df.dropna(subset=[\"features_string_format\"], inplace=True)\n",
    "            print(\"shape of final dataframe when rows that have null values where dropped: \", self.df.shape)\n",
    "        else:\n",
    "            if self.sents != None:\n",
    "                for columns in self.sents:\n",
    "                    self.df[columns] = self.df[columns].fillna('neutral')\n",
    "\n",
    "        \n",
    "    def preprocessing_steps(self):\n",
    "        # substitute tokens\n",
    "        self.initial_texts = [self.substitute(text.lower()) for text in self.df[self.column]]\n",
    "        \n",
    "        # tokenize the text\n",
    "        self.initial_tokens = [self.tokenizer.tokenize(text) for text in self.initial_texts]\n",
    "        \n",
    "        # remove stopwords\n",
    "        self.stopped_tokens = [[token for token in token_list if len(token) >= 2 \n",
    "                                and token not in self.stop_words] for token_list in self.initial_tokens]\n",
    "        \n",
    "        # make them in string format\n",
    "        self.stopped_text = [' '.join(tokens) for tokens in self.stopped_tokens]\n",
    "    \n",
    "    def paraphrase(self, list_of_paraphrasing: list = None, category: int = None):\n",
    "        if list_of_paraphrasing is None or category is None:\n",
    "            print('Input the list of words to be paraphrased and the category value.')\n",
    "            return\n",
    "        \n",
    "        temp_df = self.df[self.df[self.sents[0]] == category]\n",
    "\n",
    "        if temp_df.empty:\n",
    "            print(f\"No rows found for category {category}.\")\n",
    "            return\n",
    "        \n",
    "        new_rows = []  # Initialize a list to store new rows\n",
    "\n",
    "        for index, row in temp_df.iterrows():\n",
    "            original_text = row['features_string_format']  # Initialize with original text\n",
    "            paraphrased_text = original_text  # Initialize paraphrased_text\n",
    "\n",
    "            for words in list_of_paraphrasing:\n",
    "                original_word, paraphrased_word = words\n",
    "                paraphrased_text = re.sub(r'\\b{}\\b'.format(re.escape(original_word)), paraphrased_word, paraphrased_text)\n",
    "\n",
    "            if paraphrased_text != original_text:\n",
    "                # If there's a paraphrased text, create a new row with it\n",
    "                new_row = row.copy()\n",
    "                new_row['features_string_format'] = paraphrased_text\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "        # Create a new DataFrame using the list of new rows\n",
    "        paraphrased_df = pd.DataFrame(new_rows, columns=temp_df.columns)\n",
    "\n",
    "        # Return the new DataFrame\n",
    "        return paraphrased_df\n",
    "    \n",
    "    def get_pattern_data(self,pattern=None,sent=None):\n",
    "        if pattern==None:\n",
    "            print('Input the word/s of the data you want to get')\n",
    "            return\n",
    "        elif type(pattern) != str:\n",
    "            print('Input the word/s of the data you want to get')\n",
    "            return\n",
    "        expression = r'\\b{}\\b'.format(pattern)\n",
    "        filtered_comments = self.df[self.df[self.column].str.contains(expression, case=False, regex=True)]\n",
    "        \n",
    "        if sent==None:\n",
    "            print(1)\n",
    "            return filtered_comments\n",
    "        elif type(sent) != int:\n",
    "            print('Choose only between 0 to 2.')\n",
    "            return\n",
    "        elif sent in [0,1,2]:\n",
    "            print(sent)\n",
    "            data = filtered_comments[filtered_comments[self.sents] == sent]\n",
    "            print(filtered_comments)\n",
    "            return data\n",
    "        \n",
    "    def substitute(self, text):\n",
    "        text = re.sub(r'([?!])+', r'\\1', text)\n",
    "        text = re.sub(r'@[\\w\\.]+', '', text, flags=re.IGNORECASE)  # Remove TikTok tags starting with '@'\n",
    "        text = re.sub(r'#\\w+', '', text, flags=re.IGNORECASE)      # Remove hashtags starting with '#'\n",
    "        text = re.sub(r'\\S*@\\S*', '', text, flags=re.IGNORECASE)   # Remove any remaining email-like patterns\n",
    "        text = re.sub(r'\\b(?:http\\S+|@\\S+)\\b', '', text, flags=re.IGNORECASE)  # Remove URLs and remaining '@' patterns\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\?\\!\\.\\,]', '', text)  # Remove special characters except for punctuation\n",
    "        text = text.replace('\\n', ' ')        # Replace newline characters with spaces\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\?\\!\\.\\,]', '', text)  # Remove special characters again (duplicate line)\n",
    "        text = re.sub(r'\\b\\w*haha\\w*\\b', 'hahahaha', text, flags=re.IGNORECASE)  # Replace variations of 'haha' with 'hahahaha'\n",
    "        text = re.sub(r'bomata', 'bumata', text)\n",
    "        text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "        text = re.sub(r'bbo', 'bobo', text)\n",
    "        text = re.sub(r'tnga', 'tanga', text)\n",
    "        text = re.sub(r'pnget', 'panget', text)\n",
    "        text = re.sub(r'thanks', 'thank', text)\n",
    "        text = re.sub(r'\\bpoh\\b', 'po', text)\n",
    "        text = re.sub(r'\\sno\\b', 'sino', text)\n",
    "        text = re.sub(r'\\bparents\\b', 'parent', text)\n",
    "        text = re.sub(r'\\bpano\\b', 'paano', text)\n",
    "        text = re.sub(r'\\bpanu\\b', 'paano', text)\n",
    "        text = re.sub(r'\\bpno\\b', 'paano', text)\n",
    "        text = re.sub(r'\\bpaanu\\b', 'paano', text)\n",
    "        text = re.sub(r'\\bnman\\b', 'naman', text)\n",
    "        text = re.sub(r'\\bnmn\\b', 'naman', text)\n",
    "        text = re.sub(r'\\bkhit\\b', 'kahit', text)\n",
    "        text = re.sub(r'\\bkht\\b', 'kahit', text)\n",
    "        text = re.sub(r'\\bpwedi\\b', 'pwede', text)\n",
    "        text = re.sub(r'\\bpwde\\b', 'pwede', text)\n",
    "        text = re.sub(r'\\bpede\\b', 'pwede', text)\n",
    "        text = re.sub(r'\\byong\\b', 'yung', text)\n",
    "        text = re.sub(r'\\bkasu\\b', 'kaso', text)\n",
    "        text = re.sub(r'\\bkso\\b', 'kaso', text)\n",
    "        text = re.sub(r'\\bhndi\\b', 'hindi', text)\n",
    "        text = re.sub(r'\\bd\\b', 'hindi', text)\n",
    "        text = re.sub(r'\\bsan\\b', 'saan', text)\n",
    "        text = re.sub(r'\\baplay\\b', 'apply', text)\n",
    "        text = re.sub(r'\\baply\\b', 'apply', text)\n",
    "        text = re.sub(r'\\bappy\\b', 'apply', text)\n",
    "        text = re.sub(r'ngaun', 'ngayon', text)\n",
    "        text = re.sub(r'\\bnla\\b', 'nila', text)\n",
    "        text = re.sub(r'nakha', 'nakuha', text)\n",
    "        text = re.sub(r'\\baqo\\b', 'ako', text)\n",
    "        text = re.sub(r'\\baq\\b', 'ako', text)\n",
    "        text = re.sub(r'aqoeng', 'akong', text)\n",
    "        text = re.sub(r'\\bala\\b', 'wala', text)\n",
    "        text = re.sub(r'walang', 'wala', text)\n",
    "        text = re.sub(r'\\bwla\\b', 'wala', text)\n",
    "        text = re.sub(r'\\bwalng\\b', 'wala', text)\n",
    "        text = re.sub(r'\\bwlang\\b', 'wala', text)\n",
    "        text = re.sub(r'\\bmam\\b', 'maam', text)\n",
    "        text = re.sub(r'\\bbkit\\b', 'bakit', text)\n",
    "        text = re.sub(r'perent', 'parent', text)\n",
    "        text = re.sub(r'\\bpo b\\b', 'po ba', text)\n",
    "        text = re.sub(r'\\bganun\\b', 'ganoon', text)\n",
    "        text = re.sub(r'\\bganon\\b', 'ganoon', text)\n",
    "        text = re.sub(r'\\bgnon\\b', 'ganoon', text)\n",
    "        text = re.sub(r'\\bnde\\b', 'hindi', text)\n",
    "        text = re.sub(r'\\bndi\\b', 'hindi', text)\n",
    "        text = re.sub(r'\\bdi\\b', 'hindi', text)\n",
    "        text = re.sub(r'\\bd\\b', 'hindi', text)\n",
    "        text = re.sub(r'\\bcnbi\\b', 'sinabi', text)\n",
    "        text = re.sub(r'\\bkyo\\b', 'kayo', text)\n",
    "        text = re.sub(r'\\btyo\\b', 'tayo', text)\n",
    "        text = re.sub(r'\\btau\\b', 'tayo', text)\n",
    "        text = re.sub(r'\\bbat\\b', 'bakit', text)\n",
    "        text = re.sub(r'\\bnakha\\b', 'nakuha', text)\n",
    "        text = re.sub(r'\\bnkuha\\b', 'nakuha', text)\n",
    "        text = re.sub(r'\\bnkha\\b', 'nakuha', text)\n",
    "        text = re.sub(r'\\bmrami\\b', 'marami', text)\n",
    "        text = re.sub(r'\\bmdami\\b', 'marami', text)\n",
    "        text = re.sub(r'\\btlaga\\b', 'talaga', text)\n",
    "        text = re.sub(r'\\btlaaga\\b', 'talaga', text)\n",
    "        text = re.sub(r'\\bq\\b', 'ako', text)\n",
    "        text = re.sub(r'\\bkaayu\\b', 'kayo', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfecd3",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This class is use to: \n",
    "\n",
    "    -explore the data\n",
    "    -find insights within the data \n",
    "    -outputs the visualization that would be useful for preprocessing and presentation purposes.\n",
    "\n",
    "    The following are the outputs:\n",
    "        -descriptions (i.e. categories, number of values, mean)\n",
    "        -wordcloud\n",
    "        -sentiment plots\n",
    "        -bar plots\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "099f73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eda:\n",
    "    def __init__(self, df, text_column_name: str, sent_column_name: str = None, folder: str = None, background_color: str = '#FFFFFF', text_color: str = '#000000', bar_color: str = '#121166'):\n",
    "        self.text_column_name = text_column_name\n",
    "        self.sent_column_name = sent_column_name\n",
    "        self.folder = folder\n",
    "        self.df = df\n",
    "        self.wordcloud = WordCloud(width=1200, height=800, max_words=500, background_color=\"white\", scale=2)\n",
    "        self.all_bigrams = []\n",
    "        self.all_text = ' '.join(self.df[self.text_column_name])\n",
    "        self.background_color = background_color\n",
    "        self.text_color = text_color\n",
    "        self.bar_color = bar_color\n",
    "        self.filtered_comments = None\n",
    "        self.all_grams = []\n",
    "        self.sentiment_dict = {                            # switch name into integers\n",
    "                'negative': 0,\n",
    "                'positive': 2,\n",
    "                'neutral': 1,\n",
    "            }\n",
    "        if self.sent_column_name != None:\n",
    "            # map the sentiments into 0,1,2\n",
    "            self.df[self.sent_column_name] = self.df[self.sent_column_name].apply(lambda x: x.lower() if isinstance(x, str) else x).map(self.sentiment_dict)\n",
    "            print(self.df[self.sent_column_name].value_counts())\n",
    "        if self.folder != None:\n",
    "            if not os.path.isdir(self.folder):\n",
    "                print('File doesn\\'t exist.')\n",
    "\n",
    "    def descriptions(self):\n",
    "        print('shape:\\n', self.df.shape, '\\n')\n",
    "        if self.sent_column_name != None:\n",
    "            print('sentiment count:\\n', self.df[self.sent_column_name].value_counts(), '\\n')\n",
    "        print('columns:', list(self.df.columns.values), '\\n')\n",
    "\n",
    "    def word_number(self):\n",
    "        word_counts = self.df[self.text_column_name].str.split().apply(len)\n",
    "        bins = np.arange(1, max(word_counts) + 2)  # Start from 1 and end at max + 1\n",
    "        plt.hist(word_counts, bins=bins)\n",
    "        plt.xlabel('Number of Words')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Number of Words in Comments')\n",
    "\n",
    "        # Customize x-axis tick positions and labels\n",
    "        tick_positions = np.arange(0, 30, 5)  # Start from 1, increment by 5, end at 30\n",
    "        tick_labels = [str(pos) for pos in tick_positions]\n",
    "        plt.xticks(tick_positions, tick_labels)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def pattern_matching(self, pattern, show_sents=None, show_comments=None):\n",
    "        expression = r'\\b{}\\b'.format(pattern)  # Regular expression pattern to match the word or words\n",
    "        \n",
    "        self.filtered_comments = self.df[self.df[self.text_column_name].str.contains(expression, case=False, regex=True)]\n",
    "        \n",
    "        num_comments = len(self.filtered_comments)\n",
    "        \n",
    "        # print the total comments having the pattern\n",
    "        print(f\"Total number of comments containing '{pattern}' = {num_comments}:100%\")\n",
    "        \n",
    "        # if user want to see the percentage of each sentiments\n",
    "        if show_sents != None:\n",
    "            # calculate the number of each sentiments\n",
    "            num_negative_comments = len(self.filtered_comments[self.filtered_comments[self.sent_column_name] == 0])\n",
    "            num_positive_comments = len(self.filtered_comments[self.filtered_comments[self.sent_column_name] == 2])\n",
    "            num_neutral_comments = len(self.filtered_comments[self.filtered_comments[self.sent_column_name] == 1])\n",
    "            \n",
    "            negative_percentage = (num_negative_comments/num_comments) * 100\n",
    "            positive_percentage = (num_positive_comments/num_comments) * 100\n",
    "            neutral_percentage = (num_neutral_comments/num_comments) * 100\n",
    "            \n",
    "            print(f\"Positive = {num_positive_comments}:{positive_percentage:.3f}%\")\n",
    "            print(f\"Negative = {num_negative_comments}:{negative_percentage:.3f}%\")\n",
    "            print(f\"Neutral = {num_neutral_comments}:{neutral_percentage:.3f}%\")\n",
    "            \n",
    "        # if user want to see the comments\n",
    "        if show_comments != None:\n",
    "            print('All comments with the pattern: \\n')\n",
    "            for word in self.filtered_comments[self.text_column_name]:\n",
    "                print(word)\n",
    "        \n",
    "    def top_words(self, top: int, sentiment: str):\n",
    "        # Tokenize the text\n",
    "        tokens = self.all_text.split()\n",
    "\n",
    "        # Count the frequency of each word\n",
    "        word_counts = pd.Series(tokens).value_counts()\n",
    "\n",
    "        # Select the top 10 words\n",
    "        top_10_words = word_counts.head(top)\n",
    "\n",
    "        # Set the color specifications\n",
    "        background_color = self.background_color\n",
    "        text_color = self.text_color\n",
    "        bar_color = self.bar_color\n",
    "\n",
    "        # Create the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(11, 7), facecolor=background_color)\n",
    "        ax.barh(top_10_words.index, top_10_words.values, color=bar_color)\n",
    "\n",
    "        # Set the background and text colors\n",
    "        fig.set_facecolor(background_color)\n",
    "        ax.set_facecolor(background_color)\n",
    "        ax.xaxis.label.set_color(text_color)\n",
    "        ax.yaxis.label.set_color(text_color)\n",
    "        ax.tick_params(axis='x', colors=text_color)\n",
    "        ax.tick_params(axis='y', colors=text_color)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Frequency', color=text_color)\n",
    "        plt.ylabel('Words', color=text_color)\n",
    "        plt.title('Top {} {} Words'.format(top, sentiment), color=text_color)\n",
    "\n",
    "        # Show the chart\n",
    "        if self.folder != None:\n",
    "            plt.savefig('{}/top_{}words_{}.png'.format(self.folder, top, self.folder))\n",
    "        plt.show()\n",
    "        \n",
    "    def grams(self, top: int, sentiment: str, gram_type: str):\n",
    "        # create an empty grams\n",
    "        self.all_grams = []\n",
    "        \n",
    "        # Determine the gram size based on gram_type\n",
    "        if gram_type == 'unigram':\n",
    "            print(1)\n",
    "            gram_size = 1\n",
    "        elif gram_type == 'bigram':\n",
    "            print(2)\n",
    "            gram_size = 2\n",
    "        elif gram_type == 'trigram':\n",
    "            print(3)\n",
    "            gram_size = 3\n",
    "        else:\n",
    "            raise ValueError(\"Invalid gram_type. Supported values are 'unigram', 'bigram', and 'trigram'.\")\n",
    "\n",
    "        x = 0\n",
    "        # Iterate over each text and compute grams\n",
    "        for text in self.df[self.text_column_name]:\n",
    "            # Tokenize the text into individual words\n",
    "            tokens = text.split()\n",
    "            \n",
    "            if x == 0:\n",
    "                print(\"gram_size: \", gram_size)\n",
    "            \n",
    "            # Generate n-grams\n",
    "            grams_tokens = list(combinations(tokens, gram_size))\n",
    "\n",
    "            # Add the grams to the list\n",
    "            self.all_grams.extend(grams_tokens)\n",
    "            \n",
    "            x += 1\n",
    "\n",
    "        # Count the frequency of each gram\n",
    "        gram_counts = Counter(self.all_grams)\n",
    "\n",
    "        # Select the top n grams\n",
    "        top_n_grams = gram_counts.most_common(top)\n",
    "\n",
    "        # Extract the gram phrases and their frequencies\n",
    "        gram_phrases, gram_frequencies = zip(*top_n_grams)\n",
    "\n",
    "        # Convert the tuples to lists\n",
    "        gram_phrases = [', '.join(phrase) for phrase in gram_phrases]\n",
    "        gram_frequencies = list(gram_frequencies)\n",
    "\n",
    "        # Create the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(10, 8), facecolor=self.background_color)\n",
    "        ax.barh(gram_phrases, gram_frequencies, color=self.bar_color)\n",
    "\n",
    "        # Set the background and text colors\n",
    "        fig.set_facecolor(self.background_color)\n",
    "        ax.set_facecolor(self.background_color)\n",
    "        ax.xaxis.label.set_color(self.text_color)\n",
    "        ax.yaxis.label.set_color(self.text_color)\n",
    "        ax.tick_params(axis='x', colors=self.text_color)\n",
    "        ax.tick_params(axis='y', colors=self.text_color)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Frequency', color=self.text_color)\n",
    "        plt.ylabel(f'{gram_type.capitalize()}s', color=self.text_color)\n",
    "        plt.title(f'Top {top} {sentiment} {gram_type.capitalize()}s', color=self.text_color)\n",
    "\n",
    "        # Save chart\n",
    "        if self.folder is not None:\n",
    "            plt.savefig(f'{self.folder}/top_{top}_{gram_type}s_{self.folder}.png')\n",
    "        else:\n",
    "            plt.savefig(f'top_{top}_{gram_type}s_{self.folder}.png')\n",
    "\n",
    "        # Show the chart\n",
    "        plt.show()\n",
    "\n",
    "    def generate_wordcloud(self, value: int = None):\n",
    "        # if there is a column name given\n",
    "        if value is not None:\n",
    "            joined_comments = ''\n",
    "            # combine all the text into one string\n",
    "            for index, row in self.df.iterrows():\n",
    "                if row[self.sent_column_name] == value:\n",
    "                    joined_comments += ' ' + str(row[self.text_column_name])\n",
    "            # create cloud\n",
    "            self.wordcloud.generate(joined_comments)\n",
    "        else:\n",
    "            joined_comments = ' '.join(self.df[self.text_column_name])\n",
    "\n",
    "        # Split words\n",
    "        words = joined_comments.split()\n",
    "\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter(words)\n",
    "\n",
    "        # Get the top 10 words based on frequency\n",
    "        top_words = [word for word, count in word_counts.most_common(300)]\n",
    "\n",
    "        # Generate word cloud from the top 10 words\n",
    "        self.wordcloud.generate(' '.join(top_words))\n",
    "        \n",
    "        # Set the background color\n",
    "        background_color = self.background_color\n",
    "        self.wordcloud.background_color = background_color\n",
    "\n",
    "        # Set the colors of the words\n",
    "        if value == None:\n",
    "            word_colors = ['#331D2C', '#3F2E3E', '#3C2A21', '#400E32']\n",
    "        else:\n",
    "            # if negative\n",
    "            if value == 0:\n",
    "                word_colors = ['#900C3F', '#C70039', '#F94C10', '#FE0000']\n",
    "            # if neutral\n",
    "            elif value == 1:\n",
    "                word_colors = ['#61677A', '#0E2954', '#435B66', '#394867']\n",
    "            # if positive\n",
    "            elif value == 2:\n",
    "                word_colors = ['#285430', '#54B435', '#379237', '#82CD47']\n",
    "                \n",
    "        self.wordcloud.recolor(color_func=lambda *args, **kwargs: random.choice(word_colors))\n",
    "\n",
    "        if self.folder != None:\n",
    "            if value is not None:\n",
    "                if value == 0:\n",
    "                    self.wordcloud.to_file('{}/{}_negative_wordcloud.png'.format(self.folder, self.folder))\n",
    "                elif value == 1:\n",
    "                    self.wordcloud.to_file('{}/{}_neutral_wordcloud.png'.format(self.folder, self.folder))\n",
    "                elif value == 2:\n",
    "                    self.wordcloud.to_file('{}/{}_positive_wordcloud.png'.format(self.folder, self.folder))\n",
    "            else:\n",
    "                self.wordcloud.to_file('{}/{}_wordcloud.png'.format(self.folder, self.folder))\n",
    "        else:\n",
    "            if value is not None:\n",
    "                if value == 0:\n",
    "                    self.wordcloud.to_file('{}_negative_wordcloud.png'.format(self.folder))\n",
    "                elif value == 1:\n",
    "                    self.wordcloud.to_file('{}_neutral_wordcloud.png'.format(self.folder))\n",
    "                elif value == 2:\n",
    "                    self.wordcloud.to_file('{}_positive_wordcloud.png'.format(self.folder))\n",
    "            else:\n",
    "                self.wordcloud.to_file('{}_wordcloud.png'.format(self.folder))\n",
    "            \n",
    "        # show plot\n",
    "        plt.imshow(self.wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    def pie_sentiment(self):\n",
    "        value_counts = self.df[self.sent_column_name].value_counts()\n",
    "\n",
    "        labels = value_counts.index\n",
    "        counts = value_counts.values\n",
    "\n",
    "        colors = ['#001C30', '#176B87', '#64CCC5']\n",
    "        background_color = self.background_color\n",
    "\n",
    "        # Map the index values to the desired labels\n",
    "        sentiment_labels = labels.map({0.0: 'negative', 1.0: 'neutral', 2.0: 'positive'})\n",
    "\n",
    "        plt.figure(facecolor=background_color)\n",
    "        plt.style.use('default')\n",
    "        plt.pie(counts, labels=sentiment_labels, autopct='%1.1f%%', colors=colors)\n",
    "        plt.axis('equal')\n",
    "        plt.gca().set_facecolor(background_color)\n",
    "        if self.folder != None:\n",
    "            plt.savefig('{}/{}_pie_plot_sentiment.png'.format(self.folder, self.folder))\n",
    "        else:\n",
    "            plt.savefig('{}_pie_plot_sentiment.png'.format(self.folder))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def bar_sentiment(self):\n",
    "        # Set the color specifications\n",
    "        background_color = self.background_color\n",
    "        text_color = self.text_color\n",
    "        bar_color = self.bar_color\n",
    "\n",
    "        # Set the background color\n",
    "        plt.figure(facecolor=background_color)\n",
    "\n",
    "        # Count the occurrences of each sentiment\n",
    "        sentiment_counts = self.df[self.sent_column_name].value_counts()\n",
    "\n",
    "        # Define the color specifications\n",
    "        color_mapping = {'negative': '#D21312', 'positive': '#03C988', 'neutral': '#576CBC'}\n",
    "\n",
    "        # Map the index values to the desired labels\n",
    "        sentiment_labels = sentiment_counts.index.map({0.0: 'negative', 1.0: 'neutral', 2.0: 'positive'})\n",
    "\n",
    "        # Create a bar plot with custom colors and updated labels\n",
    "        plt.bar(sentiment_labels, sentiment_counts.values, color=[color_mapping.get(sentiment, bar_color) for sentiment in sentiment_labels])\n",
    "\n",
    "        # Set the text color\n",
    "        plt.xlabel('Sentiments', color=text_color)\n",
    "        plt.ylabel('Count', color=text_color)\n",
    "\n",
    "        # Set the title and text colors based on column_name\n",
    "        plt.title('Sentiments', color=text_color)\n",
    "\n",
    "        # Set the tick colors\n",
    "        plt.tick_params(colors=text_color)\n",
    "\n",
    "        # Set the plot area color to the same as the background color\n",
    "        plt.gca().set_facecolor(background_color)\n",
    "\n",
    "        # Display the plot\n",
    "        if self.folder != None:\n",
    "            plt.savefig('{}/{}_bar_sentiment_plot.png'.format(self.folder, self.folder))\n",
    "        else:\n",
    "            plt.savefig('{}_bar_sentiment_plot.png'.format(self.folder))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb02229",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "    this class is designed to use a pre-trained model to determine wether a text is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3af90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.fasttext_model = fasttext.load_model('C:\\\\project\\\\downloads\\\\lid.176.ftz')\n",
    "        \n",
    "        self.do_sentiments()\n",
    "        \n",
    "    def do_sentiments(self):\n",
    "        # declare a list of sentiments\n",
    "        english_sentiments = []\n",
    "        \n",
    "        for index, row in self.df.iterrows():\n",
    "            if row['language'] == \"English\":\n",
    "                # Perform sentiment analysis with VADER\n",
    "                vader_sentiment = self.vader_analyzer.polarity_scores(row['features_string_format'])\n",
    "                # Get the most probable sentiment label\n",
    "                sentiment_label = max(vader_sentiment, key=vader_sentiment.get)\n",
    "                # Map the sentiment labels to custom labels\n",
    "                if sentiment_label == \"neu\":\n",
    "                    sentiment_label = \"neutral\"\n",
    "                elif sentiment_label == \"pos\" or sentiment_label == \"compound\":\n",
    "                    sentiment_label = \"positive\"\n",
    "                else:\n",
    "                    sentiment_label = \"negative\"\n",
    "                # Update the dataframe with sentiment label\n",
    "                english_sentiments.append(sentiment_label)\n",
    "            else:\n",
    "                english_sentiments.append(None)\n",
    "        \n",
    "        # append to dataframe\n",
    "        self.df['english_sents'] = english_sentiments\n",
    "        \n",
    "    def combine_sentiments(self):\n",
    "        english_tagalog_sents = []\n",
    "        for index, row in self.df.iterrows():\n",
    "            if row['language'] == 'English':\n",
    "                english_tagalog_sents.append(row['english_sents'])\n",
    "            else:\n",
    "                english_tagalog_sents.append(row['Comment type'])\n",
    "        self.df['sentiments'] = english_tagalog_sents\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f18d86",
   "metadata": {},
   "source": [
    "# Unsupervised using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb0f7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "class LDA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.lowered_token_list = []\n",
    "        self.corpus = []\n",
    "        self.model = None\n",
    "        self.modelling()\n",
    "        \n",
    "    def preparing(self):\n",
    "        # lowercase the tokens\n",
    "        self.lowered_token_list = []\n",
    "        \n",
    "        # put the values into the empty list\n",
    "        for index, row in self.df.iterrows():\n",
    "            lowercase_tokens = []\n",
    "            for token in row['features']:\n",
    "                lowercase_tokens.append(token.lower())\n",
    "            self.lowered_token_list.append(lowercase_tokens)\n",
    "        \n",
    "        # initialize id2word here\n",
    "        self.id2word = corpora.Dictionary(self.lowered_token_list)\n",
    "        \n",
    "        # put the tokens into the corpus\n",
    "        for list_of_tokens in self.lowered_token_list:\n",
    "            new = self.id2word.doc2bow(list_of_tokens)\n",
    "            self.corpus.append(new)\n",
    "    \n",
    "    def modelling(self):\n",
    "        self.preparing()\n",
    "        self.model = models.LdaModel(\n",
    "            corpus=self.corpus,\n",
    "            id2word=self.id2word,\n",
    "            num_topics=3,\n",
    "            random_state=42,\n",
    "            update_every=1,\n",
    "            passes=10,\n",
    "            chunksize=180,\n",
    "            alpha='auto'\n",
    "        )\n",
    "    \n",
    "    def output(self):\n",
    "        pyLDAvis.enable_notebook()\n",
    "        vis = pyLDAvis.gensim_models.prepare(self.model, self.corpus, self.id2word, mds='mmds', R=30)\n",
    "        pyLDAvis.save_html(vis, 'visualization.html')  # Save the visualization to an HTML file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a082db",
   "metadata": {},
   "source": [
    "# Unsupervised using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ec1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(df['features_string_format'])\n",
    "        \n",
    "        self.min_clusters = 1\n",
    "        self.max_clusters = 6\n",
    "        self.cluster_range = range(self.min_clusters, self.max_clusters)\n",
    "        self.inertia_values = []\n",
    "        self.silhouette_scores = []\n",
    "        \n",
    "        self.find_best_cluster()\n",
    "        self.plot()\n",
    "        \n",
    "    def find_best_cluster(self):\n",
    "        for num_clusters in self.cluster_range:\n",
    "            kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "            kmeans.fit(self.tfidf_matrix)\n",
    "            self.inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "            labels = kmeans.labels_\n",
    "            num_unique_labels = len(np.unique(labels))\n",
    "\n",
    "            if num_unique_labels > 1:\n",
    "                score = silhouette_score(self.tfidf_matrix, labels)\n",
    "                self.silhouette_scores.append(score)\n",
    "            else:\n",
    "                self.silhouette_scores.append(0.0)\n",
    "        \n",
    "    def plot(self):\n",
    "        if not self.cluster_range or not self.inertia_values or not self.silhouette_scores:\n",
    "            print(\"Insufficient data to plot.\")\n",
    "            return\n",
    "    \n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.plot(self.cluster_range, self.inertia_values, marker='o', color='green')\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Inertia')\n",
    "        ax1.set_title('Elbow Curve')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(self.cluster_range, self.silhouette_scores, marker='o', color='red')\n",
    "        ax2.set_ylabel('Silhouette Score')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def update_database(self, best_cluster):\n",
    "        # cluster\n",
    "        kmeans = KMeans(n_clusters=best_cluster, init='k-means++', random_state=42)\n",
    "        kmeans.fit(self.tfidf_matrix)\n",
    "        \n",
    "        # update dataframe\n",
    "        self.df['cluster_label'] = kmeans.labels_.astype(str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb9ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class models:\n",
    "    def __init__(self, df=None, xtrain=None, xtest=None, ytrain=None, ytest=None, x: str=None, y: str=None, filename: str=None):\n",
    "        self.filename = filename\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.df = df\n",
    "        if not self.df.empty:\n",
    "            if self.x == None:\n",
    "                print('Input the column name of x data.')\n",
    "                return\n",
    "            if self.y == None:\n",
    "                print('Input the column name of y data.')\n",
    "                return\n",
    "            if self.filename == None:\n",
    "                print('Input the filename of the data.')\n",
    "                return\n",
    "            self.xtrain, self.xtest, self.ytrain, self.ytest = train_test_split(self.df[self.x], self.df[self.y], \n",
    "                                                                                test_size=.20, stratify=df[self.y])\n",
    "        elif self.df.empty:\n",
    "            self.xtrain = xtrain \n",
    "            self.xtest = xtest\n",
    "            self.ytrain = ytrain\n",
    "            self.ytest = ytest\n",
    "            if xtrain.empty:\n",
    "                print('Input the xtrain.')\n",
    "                return\n",
    "            if xtest.empty:\n",
    "                print('Input the xtest.')\n",
    "                return\n",
    "            if ytrain.empty:\n",
    "                print('Input the ytrain.')\n",
    "                return\n",
    "            if ytest.empty:\n",
    "                print('Input the ytest.')\n",
    "                return\n",
    "        else:\n",
    "            print('Input a dataframe or the training and testing data.')\n",
    "            return\n",
    "        self.model = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.xtrain_vectors = self.vectorizer.fit_transform(self.xtrain)\n",
    "        self.xtest_vectors = self.vectorizer.transform(self.xtest)\n",
    "        self.ytest = self.ytest.astype(int)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.ytrain)\n",
    "        self.model_name = None\n",
    "        self.y_predicted = None\n",
    "        \n",
    "    def bert_classification(self, epochs: int):\n",
    "        self.model_name = 'bert'\n",
    "        # Model framework\n",
    "        e_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "        p_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "        \n",
    "        bert_preprocess = hub.KerasLayer(p_url)\n",
    "        bert_encoder = hub.KerasLayer(e_url)\n",
    "        \n",
    "        article = tf.keras.layers.Input(shape=(), dtype=tf.string, name='article')\n",
    "        p_text = bert_preprocess(article)\n",
    "        output = bert_encoder(p_text)\n",
    "        l = tf.keras.layers.BatchNormalization()(output['pooled_output'])\n",
    "        l = tf.keras.layers.Dropout(0.1, name='dropout_1')(l)\n",
    "        l = tf.keras.layers.Dense(128, activation='relu')(l)\n",
    "        l = tf.keras.layers.Dropout(0.1, name='dropout')(l)\n",
    "        l = tf.keras.layers.Dense(3, activation='softmax', name='output')(l)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[article], outputs=[l])\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                           loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Start the time checker of the program\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(self.xtrain, self.ytrain, epochs=epochs)\n",
    "        \n",
    "        # End time\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Print the elapsed time\n",
    "        print('Elapsed time:', elapsed_time / 60)\n",
    "        \n",
    "        self.model.save('bert_model_{}.h5'.format(self.filename))\n",
    "        \n",
    "    def svm(self):\n",
    "        self.model_name = 'svm'\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],  # Penalty parameter C\n",
    "            'kernel': ['linear', 'rbf', 'poly'],  # Kernel type\n",
    "            'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf' and 'poly' kernels\n",
    "        }\n",
    "        clf_svm = svm.SVC(kernel='linear')\n",
    "        \n",
    "        # Instantiate GridSearchCV\n",
    "        grid_search = GridSearchCV(clf_svm, param_grid, cv=10)  # 5-fold cross-validation\n",
    "        \n",
    "        # Fit the gridsearch to the data\n",
    "        grid_search.fit(self.xtrain_vectors, self.ytrain)\n",
    "        \n",
    "        # Access the best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        # Create a new SVM model with the best parameters\n",
    "        best_svm_model = svm.SVC(**best_params)\n",
    "    \n",
    "        # Train the best SVM model on the entire training data\n",
    "        best_svm_model.fit(self.xtrain_vectors, self.ytrain)\n",
    "\n",
    "        # save model to class\n",
    "        self.model = best_svm_model\n",
    "        \n",
    "        # Save the model to a file\n",
    "        joblib.dump(best_svm_model, 'svm_model_{}.pkl'.format(self.filename))\n",
    "\n",
    "        # Save the vectorizer\n",
    "        joblib.dump(self.vectorizer, 'svm_vectorizer_{}.pkl'.format(self.filename))\n",
    "\n",
    "        # Save the label encoder\n",
    "        joblib.dump(self.label_encoder, 'svm_label_encoder_{}.pkl'.format(self.filename))\n",
    "        \n",
    "    def decision_tree(self):\n",
    "        self.model_name = 'dectree'\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {\n",
    "            'criterion': ['gini', 'entropy'],         # Split criterion\n",
    "            'splitter': ['best', 'random'],          # Strategy for choosing split\n",
    "            'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
    "            'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "            'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "            'max_features': ['sqrt', 'log2'] # Number of features to consider for the best split\n",
    "        }\n",
    "\n",
    "        # Create the Decision Tree model\n",
    "        dt_model = DecisionTreeClassifier()\n",
    "\n",
    "        # Instantiate GridSearchCV\n",
    "        grid_search = GridSearchCV(dt_model, param_grid, cv=10)  # 5-fold cross-validation\n",
    "\n",
    "        # Fit GridSearchCV to the data\n",
    "        grid_search.fit(self.xtrain_vectors, self.ytrain)\n",
    "\n",
    "        # Access the best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Create a new Decision Tree model with the best parameters\n",
    "        best_dt_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "        # Train the best Decision Tree model on the entire training data\n",
    "        best_dt_model.fit(self.xtrain_vectors, self.ytrain)\n",
    "        \n",
    "        self.model = best_dt_model\n",
    "        \n",
    "        # Save the model to a file\n",
    "        joblib.dump(best_dt_model, 'dectree_model_{}.pkl'.format(self.filename))\n",
    "\n",
    "        # Save the vectorizer\n",
    "        joblib.dump(self.vectorizer, 'dectree_vectorizer_{}.pkl'.format(self.filename))\n",
    "\n",
    "        # Save the label encoder\n",
    "        joblib.dump(self.label_encoder, 'dectree_encoder_{}.pkl'.format(self.filename))\n",
    "        \n",
    "    def random_forest(self):\n",
    "        self.model_name = 'rf'\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],          # Number of trees in the forest\n",
    "            'criterion': ['gini', 'entropy'],         # Split criterion for individual trees\n",
    "            'max_depth': [None, 10, 20, 30],         # Maximum depth of the individual trees\n",
    "            'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "            'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "            'max_features': ['sqrt', 'log2'] # Number of features to consider for the best split\n",
    "        }\n",
    "\n",
    "        # Create the Random Forest model\n",
    "        rf_model = RandomForestClassifier()\n",
    "\n",
    "        # Instantiate GridSearchCV\n",
    "        grid_search = GridSearchCV(rf_model, param_grid, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "        # Fit GridSearchCV to the data\n",
    "        grid_search.fit(self.xtrain_vectors, self.ytrain)\n",
    "\n",
    "        # Access the best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Create a new Random Forest model with the best parameters\n",
    "        best_rf_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "        # Train the best Random Forest model on the entire training data\n",
    "        best_rf_model.fit(self.xtrain_vectors, self.ytrain)\n",
    "        \n",
    "        self.model = best_rf_model\n",
    "        \n",
    "        # Save the model to a file\n",
    "        joblib.dump(best_rf_model, 'dectree_model_{}.pkl'.format(self.filename))\n",
    "\n",
    "        # Save the vectorizer\n",
    "        joblib.dump(self.vectorizer, 'dectree_vectorizer_{}.pkl'.format(self.filename))\n",
    "\n",
    "        # Save the label encoder\n",
    "        joblib.dump(self.label_encoder, 'dectree_encoder_{}.pkl'.format(self.filename))\n",
    "\n",
    "\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        # if there are no models\n",
    "        if self.model_name is None:\n",
    "            print('You need to train a model first')\n",
    "            return\n",
    "        \n",
    "        # if a bert model was used\n",
    "        if self.model_name == 'bert':\n",
    "            print('bert')\n",
    "            prediction = self.model.predict(self.xtest)\n",
    "            target_names = ['1.0', '2.0', '0']\n",
    "            y_preds = np.argmax(prediction, axis=1)\n",
    "            print(\"Classification Report: \\n\", classification_report(self.ytest, y_preds, target_names=target_names))\n",
    "        # else if a svm or decision tree model was used\n",
    "        elif self.model_name in [\"svm\", \"dectree\", \"rf\"]:\n",
    "            print('svm or dectree')\n",
    "            predictions = self.model.predict(self.xtest_vectors)\n",
    "            accuracy = accuracy_score(self.ytest, predictions)\n",
    "            f1_micro = f1_score(self.ytest, predictions, average='micro')\n",
    "            f1_macro = f1_score(self.ytest, predictions, average='macro')\n",
    "            classification_rep = classification_report(self.ytest, predictions)\n",
    "            \n",
    "            # Print scores and classification report\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"F1 score (micro):\", f1_micro)\n",
    "            print(\"F1 score (macro):\", f1_macro)\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_rep)\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            cm = confusion_matrix(self.ytest, predictions)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.xlabel('Predicted Labels')\n",
    "            plt.ylabel('Actual Labels')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('The models that can be used are \"bert\", \"svm\", \"dectree\", \"rf\". Choose one.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7922cd9",
   "metadata": {},
   "source": [
    "# end of classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
