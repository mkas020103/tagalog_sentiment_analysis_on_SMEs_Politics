{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55416ea",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8911d3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mdfl0\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mdfl0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import wordcloud\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "import fasttext\n",
    "import re\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import fasttext\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import gensim.models.ldamodel\n",
    "import pyLDAvis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import time\n",
    "import random\n",
    "#from gensim.model import CoherenceModel\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# modelling imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cac80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress DeprecationWarning from Pillow\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332e8e1",
   "metadata": {},
   "source": [
    "# Read data\n",
    "This class is use to: \n",
    "\n",
    "    -read the csv file on the given data and the filipino stopwords\n",
    "    -pre-clean the data by dropping unnecesary columns and null values.\n",
    "    -split the filipino text and english text\n",
    "    \n",
    "    The parameters are:\n",
    "        -filename (string format)\n",
    "        -stopwords (string format) (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd93e751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class base:\n",
    "    def __init__(self, df: str, added_stopwords:str = None):\n",
    "        self.stop_words_filename = added_stopwords\n",
    "        self.df_filename = df\n",
    "        self.df = None\n",
    "        self.stopwords_df = None\n",
    "        self.stopwords = []\n",
    "        \n",
    "        # read file\n",
    "        self.reading()\n",
    "        # pre-clean dataframe\n",
    "        self.clean()\n",
    "        \n",
    "    def reading(self):\n",
    "        # read stopwords csv\n",
    "        if stopwords is not None:\n",
    "            self.stopwords_df = pd.read_csv(self.stop_words_filename)\n",
    "            self.stopwords = [word for word in self.stopwords_df['.stopwords']]\n",
    "        else:\n",
    "            self.stopwords_df = None\n",
    "        # read csv file\n",
    "        self.df = pd.read_csv(self.df_filename)\n",
    "        \n",
    "    def clean(self):\n",
    "        # drop duplicate values\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        # drop null values\n",
    "        self.df = self.df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ce117",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9778e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess(base):\n",
    "    def __init__(self, df, added_stopwords, column: str, drop : list = None):\n",
    "        self.columns_to_drop = drop\n",
    "        self.column = column\n",
    "        super().__init__(df, added_stopwords)\n",
    "        self.stop_words = stopwords.words('english')       # words to stop\n",
    "        self.stopwords.extend(self.stop_words)             # combine all the stopwords\n",
    "        self.tokenizer = RegexpTokenizer(r\"\\w+|[^\\w\\s]+\")  # tokenizer\n",
    "        self.fasttext_model = fasttext.load_model('C:\\\\project\\\\downloads\\\\lid.176.ftz')\n",
    "        self.substituted_text = []                         # empty list of to be substituted text\n",
    "        self.initial_tokens = []                           # empty list of initial tokens\n",
    "        self.stopped_tokens = []                           # empty list of tokens without the stopwords\n",
    "        self.stopped_text = []                             # empty list of tokens in string format\n",
    "        \n",
    "        if self.columns_to_drop != None:\n",
    "            self.df.drop(self.columns_to_drop, axis = 1, inplace = True) \n",
    "        \n",
    "        # preprocess\n",
    "        self.preprocessing_steps()\n",
    "        \n",
    "        # append to dataframe\n",
    "        self.df[\"features\"] = self.stopped_tokens\n",
    "        self.df[\"features_string_format\"] = self.stopped_text\n",
    "        \n",
    "        # check if the text is english or not\n",
    "        self.language_label()\n",
    "        \n",
    "        # drop rows that have no values\n",
    "        self.df.drop(self.df[self.df['features_string_format'] == ''].index, inplace=True)\n",
    "\n",
    "        \n",
    "    def preprocessing_steps(self):\n",
    "        # substitute the unnecesary text to \"\"\n",
    "        self.substituted_text = [self.substitute(text).lower() for text in self.df[self.column]]\n",
    "        \n",
    "        # tokenize the text\n",
    "        self.initial_tokens = [self.tokenizer.tokenize(text) for text in self.substituted_text]\n",
    "        \n",
    "        # remove stopwords\n",
    "        self.stopped_tokens = [[token for token in token_list if len(token) > 2 and token not in self.stopwords] for token_list in self.initial_tokens]\n",
    "        \n",
    "        # make them in string format\n",
    "        self.stopped_text = [' '.join(tokens) for tokens in self.stopped_tokens]\n",
    "        \n",
    "        \n",
    "    def substitute(self, text):\n",
    "        text = re.sub(r'\\S*@\\S*', '', text)\n",
    "        text = re.sub(r'\\b(?:http\\S+|@\\S+)\\b', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\?\\!\\.\\,]', '', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\?\\!\\.\\,]', '', text)\n",
    "        text = re.sub(r'\\b\\w*haha\\w*\\b', 'hahahaha', text)\n",
    "        text = re.sub(r'bomata', 'bumata', text)\n",
    "        text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def language_label(self):\n",
    "        language_labels = []\n",
    "        for i, text in enumerate(self.df['features_string_format']):\n",
    "            lang_label = self.fasttext_model.predict(text)[0][0]\n",
    "            if lang_label == '__label__en':\n",
    "                language_labels.append('English')\n",
    "            else:\n",
    "                language_labels.append('Tagalog')\n",
    "        self.df['language'] = language_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfecd3",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This class is use to: \n",
    "\n",
    "    -explore the data\n",
    "    -find insights within the data \n",
    "    -outputs the visualization that would be useful for preprocessing and presentation purposes.\n",
    "\n",
    "    The following are the outputs:\n",
    "        -descriptions (i.e. categories, number of values, mean)\n",
    "        -wordcloud\n",
    "        -sentiment plots\n",
    "        -bar plots\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099f73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eda:\n",
    "    def __init__(self, df, text_column_name: str, sent_column_name: str):\n",
    "        self.text_column_name = text_column_name\n",
    "        self.sent_column_name = sent_column_name\n",
    "        self.df = df\n",
    "        self.wordcloud = WordCloud(width=1200, height=800, max_words=500, background_color=\"white\", scale=2)\n",
    "        self.all_bigrams = []\n",
    "        self.all_text = ' '.join(self.df[self.text_column_name])\n",
    "\n",
    "    def descriptions(self):\n",
    "        print('shape:\\n', self.df.shape, '\\n')\n",
    "        print('sentiment count:\\n', self.df[self.sent_column_name].value_counts(), '\\n')\n",
    "        print('columns:', list(self.df.columns.values), '\\n')\n",
    "\n",
    "    def word_number(self):\n",
    "        word_counts = self.df[self.text_column_name].str.split().apply(len)\n",
    "        bins = np.arange(1, max(word_counts) + 2)  # Start from 1 and end at max + 1\n",
    "        plt.hist(word_counts, bins=bins)\n",
    "        plt.xlabel('Number of Words')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Number of Words in Comments')\n",
    "\n",
    "        # Customize x-axis tick positions and labels\n",
    "        tick_positions = np.arange(0, 30, 5)  # Start from 1, increment by 5, end at 30\n",
    "        tick_labels = [str(pos) for pos in tick_positions]\n",
    "        plt.xticks(tick_positions, tick_labels)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def top_10words(self):\n",
    "        # Tokenize the text\n",
    "        tokens = self.all_text.split()\n",
    "\n",
    "        # Count the frequency of each word\n",
    "        word_counts = pd.Series(tokens).value_counts()\n",
    "\n",
    "        # Select the top 10 words\n",
    "        top_10_words = word_counts.head(10)\n",
    "\n",
    "        # Set the color specifications\n",
    "        background_color = '#27374D'\n",
    "        text_color = '#A5D7E8'\n",
    "        bar_color = '#1C82AD'\n",
    "\n",
    "        # Create the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(8, 6), facecolor=background_color)\n",
    "        ax.barh(top_10_words.index, top_10_words.values, color=bar_color)\n",
    "\n",
    "        # Set the background and text colors\n",
    "        fig.set_facecolor(background_color)\n",
    "        ax.set_facecolor(background_color)\n",
    "        ax.xaxis.label.set_color(text_color)\n",
    "        ax.yaxis.label.set_color(text_color)\n",
    "        ax.tick_params(axis='x', colors=text_color)\n",
    "        ax.tick_params(axis='y', colors=text_color)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Frequency', color=text_color)\n",
    "        plt.ylabel('Words', color=text_color)\n",
    "        plt.title('Top 10 Words', color=text_color)\n",
    "\n",
    "        # Show the chart\n",
    "        plt.savefig('top_10words.png')\n",
    "        plt.show()\n",
    "        \n",
    "    def bigrams(self):\n",
    "        # Iterate over each text and compute bigrams\n",
    "        for text in self.df[self.text_column_name]:\n",
    "            # Tokenize the text into individual words\n",
    "            tokens = text.split()\n",
    "\n",
    "            # Generate bigrams\n",
    "            bigram_tokens = list(combinations(tokens, 2))\n",
    "\n",
    "            # Add the bigrams to the list\n",
    "            self.all_bigrams.extend(bigram_tokens)\n",
    "\n",
    "        # Count the frequency of each bigram\n",
    "        bigram_counts = Counter(self.all_bigrams)\n",
    "\n",
    "        # Select the top 10 bigrams\n",
    "        top_10_bigrams = bigram_counts.most_common(10)\n",
    "\n",
    "        # Extract the bigram phrases and their frequencies\n",
    "        bigram_phrases, bigram_frequencies = zip(*top_10_bigrams)\n",
    "\n",
    "        # Convert the tuples to lists\n",
    "        bigram_phrases = [', '.join(phrase) for phrase in bigram_phrases]\n",
    "        bigram_frequencies = list(bigram_frequencies)\n",
    "\n",
    "        # Set the color specifications\n",
    "        background_color = '#27374D'\n",
    "        text_color = '#A5D7E8'\n",
    "        bar_color = '#1C82AD'\n",
    "\n",
    "        # Create the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(8, 6), facecolor=background_color)\n",
    "        ax.barh(bigram_phrases, bigram_frequencies, color=bar_color)\n",
    "\n",
    "        # Set the background and text colors\n",
    "        fig.set_facecolor(background_color)\n",
    "        ax.set_facecolor(background_color)\n",
    "        ax.xaxis.label.set_color(text_color)\n",
    "        ax.yaxis.label.set_color(text_color)\n",
    "        ax.tick_params(axis='x', colors=text_color)\n",
    "        ax.tick_params(axis='y', colors=text_color)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Frequency', color=text_color)\n",
    "        plt.ylabel('Bigrams', color=text_color)\n",
    "        plt.title('Top 10 Bigrams', color=text_color)\n",
    "\n",
    "        # Show the chart\n",
    "        plt.savefig('top_10bigrams.png')\n",
    "        plt.show()\n",
    "        \n",
    "    def english_tagalog(self):\n",
    "        # Count the occurrences of each language\n",
    "        language_counts = df['language'].value_counts()\n",
    "        \n",
    "        # Plotting the bar graph\n",
    "        plt.bar(language_counts.index, language_counts.values)\n",
    "\n",
    "        # Adding labels and title\n",
    "        plt.xlabel('Language')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Language Distribution')\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    def generate_wordcloud(self, filename: str, category: str = None, value: int = None):\n",
    "        if category is not None:\n",
    "            if value is None:\n",
    "                print(\"Input category value.\")\n",
    "                return\n",
    "            joined_comments = ''\n",
    "            for index, row in self.df.iterrows():\n",
    "                if row[category] == value:\n",
    "                    joined_comments += ' ' + str(row[self.text_column_name])\n",
    "            self.wordcloud.generate(joined_comments)\n",
    "        else:\n",
    "            joined_comments = ' '.join(self.df[self.text_column_name])\n",
    "            self.wordcloud.generate(joined_comments)\n",
    "\n",
    "        # Set the background color\n",
    "        background_color = '#27374D'\n",
    "        self.wordcloud.background_color = background_color\n",
    "\n",
    "        # Set the colors of the words\n",
    "        word_colors = ['#F2CA19', '#FF00BD', '#87E911', '#F3D568']\n",
    "        self.wordcloud.recolor(color_func=lambda *args, **kwargs: random.choice(word_colors))\n",
    "\n",
    "        self.wordcloud.to_file('{}_wordcloud.png'.format(filename))\n",
    "        plt.imshow(self.wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    def pie_sentiment(self, filename, column_name):\n",
    "        value_counts = self.df[column_name].value_counts()\n",
    "        \n",
    "        labels = value_counts.index\n",
    "        counts = value_counts.values\n",
    "        \n",
    "        colors = ['#001C30', '#176B87', '#64CCC5']\n",
    "        background_color = '#27374D'\n",
    "        \n",
    "        plt.figure(facecolor=background_color)\n",
    "        plt.style.use('default')\n",
    "        plt.pie(counts, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "        plt.axis('equal')\n",
    "        plt.gca().set_facecolor(background_color)\n",
    "        plt.savefig('{}_pie_plot_sentiment.png'.format(filename))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def bar_sentiment(self, column_name: str, filename: str):\n",
    "        # Set the color specifications\n",
    "        background_color = '#27374D'\n",
    "        text_color = '#A5D7E8'\n",
    "        bar_color = '#1C82AD'\n",
    "\n",
    "        # Set the background color\n",
    "        plt.figure(facecolor=background_color)\n",
    "\n",
    "        # Count the occurrences of each sentiment\n",
    "        sentiment_counts = self.df[column_name].value_counts()\n",
    "\n",
    "        # Define the color specifications\n",
    "        color_mapping = {'negative': '#D21312', 'positive': '#03C988', 'neutral': '#576CBC'}\n",
    "\n",
    "        # Create a bar plot with custom colors\n",
    "        plt.bar(sentiment_counts.index, sentiment_counts.values, color=[color_mapping.get(sentiment, bar_color) for sentiment in sentiment_counts.index])\n",
    "\n",
    "        # Set the text color\n",
    "        plt.xlabel('Sentiments', color=text_color)\n",
    "        plt.ylabel('Count', color=text_color)\n",
    "\n",
    "        # Set the title and text colors based on column_name\n",
    "        if column_name == 'Comment type':\n",
    "            plt.title('Given Sentiments', color=text_color)\n",
    "        elif column_name == 'sentiments':\n",
    "            plt.title('Tagalog & English Sentiments', color=text_color)\n",
    "        elif column_name == 'english_sents':\n",
    "            plt.title('English Sentiments', color=text_color)\n",
    "\n",
    "        # Set the tick colors\n",
    "        plt.tick_params(colors=text_color)\n",
    "\n",
    "        # Set the plot area color to the same as the background color\n",
    "        plt.gca().set_facecolor(background_color)\n",
    "\n",
    "        # Set the y-axis limits\n",
    "        plt.ylim(0, 1600)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.savefig('{}_sentiment_plot.png'.format(filename))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb02229",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "    this class is designed to use a pre-trained model to determine wether a text is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a3af90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.fasttext_model = fasttext.load_model('C:\\\\project\\\\downloads\\\\lid.176.ftz')\n",
    "        \n",
    "        self.do_sentiments()\n",
    "        \n",
    "    def do_sentiments(self):\n",
    "        # declare a list of sentiments\n",
    "        english_sentiments = []\n",
    "        \n",
    "        for index, row in self.df.iterrows():\n",
    "            if row['language'] == \"English\":\n",
    "                # Perform sentiment analysis with VADER\n",
    "                vader_sentiment = self.vader_analyzer.polarity_scores(row['features_string_format'])\n",
    "                # Get the most probable sentiment label\n",
    "                sentiment_label = max(vader_sentiment, key=vader_sentiment.get)\n",
    "                # Map the sentiment labels to custom labels\n",
    "                if sentiment_label == \"neu\":\n",
    "                    sentiment_label = \"neutral\"\n",
    "                elif sentiment_label == \"pos\" or sentiment_label == \"compound\":\n",
    "                    sentiment_label = \"positive\"\n",
    "                else:\n",
    "                    sentiment_label = \"negative\"\n",
    "                # Update the dataframe with sentiment label\n",
    "                english_sentiments.append(sentiment_label)\n",
    "            else:\n",
    "                english_sentiments.append(None)\n",
    "        \n",
    "        # append to dataframe\n",
    "        self.df['english_sents'] = english_sentiments\n",
    "        \n",
    "    def combine_sentiments(self):\n",
    "        english_tagalog_sents = []\n",
    "        for index, row in self.df.iterrows():\n",
    "            if row['language'] == 'English':\n",
    "                english_tagalog_sents.append(row['english_sents'])\n",
    "            else:\n",
    "                english_tagalog_sents.append(row['Comment type'])\n",
    "        self.df['sentiments'] = english_tagalog_sents\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f18d86",
   "metadata": {},
   "source": [
    "# Unsupervised using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb0f7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "class LDA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.lowered_token_list = []\n",
    "        self.corpus = []\n",
    "        self.model = None\n",
    "        self.modelling()\n",
    "        \n",
    "    def preparing(self):\n",
    "        # lowercase the tokens\n",
    "        self.lowered_token_list = []\n",
    "        \n",
    "        # put the values into the empty list\n",
    "        for index, row in self.df.iterrows():\n",
    "            lowercase_tokens = []\n",
    "            for token in row['features']:\n",
    "                lowercase_tokens.append(token.lower())\n",
    "            self.lowered_token_list.append(lowercase_tokens)\n",
    "        \n",
    "        # initialize id2word here\n",
    "        self.id2word = corpora.Dictionary(self.lowered_token_list)\n",
    "        \n",
    "        # put the tokens into the corpus\n",
    "        for list_of_tokens in self.lowered_token_list:\n",
    "            new = self.id2word.doc2bow(list_of_tokens)\n",
    "            self.corpus.append(new)\n",
    "    \n",
    "    def modelling(self):\n",
    "        self.preparing()\n",
    "        self.model = models.LdaModel(\n",
    "            corpus=self.corpus,\n",
    "            id2word=self.id2word,\n",
    "            num_topics=3,\n",
    "            random_state=42,\n",
    "            update_every=1,\n",
    "            passes=10,\n",
    "            chunksize=180,\n",
    "            alpha='auto'\n",
    "        )\n",
    "    \n",
    "    def output(self):\n",
    "        pyLDAvis.enable_notebook()\n",
    "        vis = pyLDAvis.gensim_models.prepare(self.model, self.corpus, self.id2word, mds='mmds', R=30)\n",
    "        pyLDAvis.save_html(vis, 'visualization.html')  # Save the visualization to an HTML file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a082db",
   "metadata": {},
   "source": [
    "# Unsupervised using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5ec1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(df['features_string_format'])\n",
    "        \n",
    "        self.min_clusters = 1\n",
    "        self.max_clusters = 6\n",
    "        self.cluster_range = range(self.min_clusters, self.max_clusters)\n",
    "        self.inertia_values = []\n",
    "        self.silhouette_scores = []\n",
    "        \n",
    "        self.find_best_cluster()\n",
    "        self.plot()\n",
    "        \n",
    "    def find_best_cluster(self):\n",
    "        for num_clusters in self.cluster_range:\n",
    "            kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "            kmeans.fit(self.tfidf_matrix)\n",
    "            self.inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "            labels = kmeans.labels_\n",
    "            num_unique_labels = len(np.unique(labels))\n",
    "\n",
    "            if num_unique_labels > 1:\n",
    "                score = silhouette_score(self.tfidf_matrix, labels)\n",
    "                self.silhouette_scores.append(score)\n",
    "            else:\n",
    "                self.silhouette_scores.append(0.0)\n",
    "        \n",
    "    def plot(self):\n",
    "        if not self.cluster_range or not self.inertia_values or not self.silhouette_scores:\n",
    "            print(\"Insufficient data to plot.\")\n",
    "            return\n",
    "    \n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.plot(self.cluster_range, self.inertia_values, marker='o', color='green')\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Inertia')\n",
    "        ax1.set_title('Elbow Curve')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(self.cluster_range, self.silhouette_scores, marker='o', color='red')\n",
    "        ax2.set_ylabel('Silhouette Score')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def update_database(self, best_cluster):\n",
    "        # cluster\n",
    "        kmeans = KMeans(n_clusters=best_cluster, init='k-means++', random_state=42)\n",
    "        kmeans.fit(self.tfidf_matrix)\n",
    "        \n",
    "        # update dataframe\n",
    "        self.df['cluster_label'] = kmeans.labels_.astype(str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb9ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class models:\n",
    "    def __init__(self, df, x: str, y: str):\n",
    "        self.df = df\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.xtrain, self.xtest, self.ytrain, self.ytest = train_test_split(self.df[self.x], self.df[self.y], \n",
    "                                                                            test_size=.20, stratify=df[self.y])\n",
    "        \n",
    "    def bert_classification(self):\n",
    "        # model framework\n",
    "        e_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "        p_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "        \n",
    "        bert_preprocess = hub.KerasLayer(p_url)\n",
    "        bert_encoder = hub.KerasLayer(e_url)\n",
    "        \n",
    "        article = tf.keras.layers.Input(shape=(), dtype=tf.string, name='article')\n",
    "        p_text = bert_preprocess(article)\n",
    "        output = bert_encoder(p_text)\n",
    "        l = tf.keras.layers.BatchNormalization()(output['pooled_output'])\n",
    "        l = tf.keras.layers.Dropout(0.1, name='dropout_1')(l)\n",
    "        l = tf.keras.layers.Dense(128, activation='relu')(l)\n",
    "        l = tf.keras.layers.Dropout(0.1, name='dropout')(l)\n",
    "        l = tf.keras.layers.Dense(3, activation='softmax', name='output')(l)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[article], outputs = [l])\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # start the time checker of the program\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train model\n",
    "        self.model.fit(xtrain, ytrain, epochs=15)\n",
    "        \n",
    "        # end time\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Print the elapsed time\n",
    "        print('Elapsed time:', elapsed_time / 60)\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        if self.model is None:\n",
    "            print('you need to train a model first')\n",
    "            return\n",
    "        \n",
    "        y_preds = self.model.predict(xtest)\n",
    "        y_preds = np.round(y_preds)\n",
    "        print(\"Classification Report: \\n\", classification_report(y_test, y_preds))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfa32f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
